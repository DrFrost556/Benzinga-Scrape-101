# -*- coding: utf-8 -*-
"""Sentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TAeGPp2zx4P9m75l3O__ih-btdvrH5NN
"""

import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import finnhub
from datetime import datetime, timedelta, timezone
import time

tokenizer = AutoTokenizer.from_pretrained("ProsusAI/finbert")
model = AutoModelForSequenceClassification.from_pretrained("ProsusAI/finbert")

# Check if GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# Move model to GPU if available
model.to(device)

companies = pd.read_csv('new_companies.csv')['ticker'].values

symbols = []
for stock in companies:
	if stock not in symbols:
		symbols.append(stock)

for stock in symbols:
	analyst = pd.read_csv(f'./benzinga_scrape/analyst_ratings/{stock}.csv',index_col=0)
	earnings = pd.read_csv(f'./benzinga_scrape/earnings_headlines/{stock}.csv',index_col=0)
	insider = pd.read_csv(f'./benzinga_scrape/insider_headlines/{stock}.csv',index_col=0)
	media = pd.read_csv(f'./benzinga_scrape/media_headlines/{stock}.csv',index_col=0)
	partner = pd.read_csv(f'./benzinga_scrape/partner_headlines/{stock}.csv',index_col=0)
	general = pd.read_csv(f'./benzinga_scrape/general_headlines/{stock}.csv',index_col=0)
	stock_df = pd.concat([analyst,earnings,insider,media,partner,general])
	stock_df['date'] = stock_df['date'].str.replace(r', \d+:\d+[AP]M$', '', regex=True)
	stock_df = stock_df[~stock_df['date'].str.contains('Sponsored')]
	stock_df['date'] = pd.to_datetime(stock_df['date'], format='%b %d, %Y')
	stock_df.set_index('date', inplace=True)
	stock_df.sort_index(inplace=True)
	stock_df = stock_df.drop('ticker', axis='columns')
	stock_df_merged = stock_df.groupby(level=0)['headline'].agg(lambda x: ' '.join(x))

# Process crawled Finnhub news articles for each ticker
for symbol in ticker:
    end_date = datetime.now(timezone.utc)
    start_date = end_date - timedelta(days=365)  # Get articles from a year back

    # Specify the file path where you want to save the sentiment scores
    json_file_path = f"/content/sample_data/{symbol}_sentiment_scores.json"  # Make sure to specify a valid file path

    sentiment_scores = []

    current_date = start_date
    while current_date <= end_date:
        next_date = current_date + timedelta(days=1)

        # Get articles for the current date
        articles = FinnhubNewsSpider().get_articles(symbol, current_date.strftime('%Y-%m-%d'), next_date.strftime('%Y-%m-%d'))

        # If there are articles, process sentiment scores
        if articles:
            input_texts = [article["headline"] for article in articles]
            combined_inputs = " ".join(input_texts)
            encoded_input = tokenizer(combined_inputs, return_tensors='pt', padding=True, truncation=True)
            encoded_input.to(device)

            # Forward pass
            with torch.no_grad():
                outputs = model(**encoded_input)

            # Post-process the output
            logits = outputs.logits
            predicted_probabilities = torch.softmax(logits, dim=1).cpu().numpy()

            # Extract positive and negative sentiment scores
            positive_sentiment_score = predicted_probabilities[0][0]
            negative_sentiment_score = predicted_probabilities[0][1]
            neutral_sentiment_score = predicted_probabilities[0][2]

            # Store sentiment scores along with the date and symbol
            sentiment_scores.append({
                'date': current_date.strftime('%Y-%m-%d'),
                'positive_sentiment_score': positive_sentiment_score,
                'negative_sentiment_score': negative_sentiment_score,
                'neutral_sentiment_score': neutral_sentiment_score
            })

        # Move to the next day
        current_date = next_date

        # Add a delay to ensure not exceeding 60 iterations per minute
        time.sleep(1)

    # Convert sentiment scores to a serializable format
    serializable_sentiment_scores = []
    for score in sentiment_scores:
        serializable_score = {
            'date': score['date'],
            'positive_sentiment_score': float(score['positive_sentiment_score']),
            'negative_sentiment_score': float(score['negative_sentiment_score']),
            'neutral_sentiment_score': float(score['neutral_sentiment_score'])
        }
        serializable_sentiment_scores.append(serializable_score)

    # Write sentiment scores to a JSON file
    json_file_path = f"{symbol}_sentiment_scores.json"
    with open(json_file_path, mode='w', encoding='utf-8') as file:
        json.dump(serializable_sentiment_scores, file, indent=4)

    print("Sentiment scores saved to:", json_file_path)
